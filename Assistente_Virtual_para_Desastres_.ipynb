{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPTp1GOtxd2gT9qzTkszKnp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dede0702/Assistente-Virtual-RAG-para-Desastres/blob/main/Assistente_Virtual_para_Desastres_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        " 1: Instala√ß√µes e Imports\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "2Qu_QijLhFlN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "R8IsHWKllAxS"
      },
      "outputs": [],
      "source": [
        "!pip install openai pandas python-dotenv -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "2: Imports e Configura√ß√£o da API Key\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Arej9t4ihvWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv # Para carregar vari√°veis de ambiente, se voc√™ usar um .env no Colab\n",
        "import getpass # Para entrada segura da API Key"
      ],
      "metadata": {
        "id": "WkC3j_RzlDoZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configura√ß√µes Iniciais ---\n",
        "\n",
        "# Tente carregar de um arquivo .env (opcional, se voc√™ fizer upload de um)\n",
        "# Crie um arquivo .env no seu Colab com a linha:\n",
        "# OPENAI_API_KEY=\"sua_chave_aqui\"\n",
        "# E fa√ßa o upload para a sess√£o do Colab.\n",
        "# Ou, use o input abaixo.\n",
        "load_dotenv()\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "if not OPENAI_API_KEY:\n",
        "    print(\"A API Key da OpenAI (OPENAI_API_KEY) n√£o foi encontrada no ambiente.\")\n",
        "    OPENAI_API_KEY = getpass.getpass(\"Por favor, insira sua OpenAI API Key e pressione Enter: \")\n",
        "\n",
        "if not OPENAI_API_KEY:\n",
        "    print(\"Erro Cr√≠tico: Nenhuma API Key da OpenAI foi fornecida. O programa n√£o pode continuar.\")\n",
        "    # Em um notebook, podemos parar a execu√ß√£o ou levantar um erro\n",
        "    # raise ValueError(\"API Key da OpenAI n√£o fornecida.\")\n",
        "    # Por ora, vamos apenas imprimir e deixar o usu√°rio lidar com isso.\n",
        "else:\n",
        "    print(\"API Key da OpenAI carregada com sucesso.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_Mom6dGhuwT",
        "outputId": "9af6cb74-cd42-4d26-8a42-df7494d3cef3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A API Key da OpenAI (OPENAI_API_KEY) n√£o foi encontrada no ambiente.\n",
            "Por favor, insira sua OpenAI API Key e pressione Enter: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "API Key da OpenAI carregada com sucesso.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modelo da OpenAI que voc√™ deseja usar\n",
        "OPENAI_MODEL_ID = \"gpt-4o-mini\" # Usando o modelo GPT-4o mini\n",
        "\n",
        "# Inicializar o cliente da API da OpenAI\n",
        "client = None\n",
        "if OPENAI_API_KEY:\n",
        "    try:\n",
        "        client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "        print(f\"Cliente OpenAI inicializado com sucesso para o modelo: {OPENAI_MODEL_ID}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao inicializar o cliente da API da OpenAI: {e}\")\n",
        "        client = None # Garante que o cliente n√£o ser√° usado se a inicializa√ß√£o falhar\n",
        "else:\n",
        "    print(\"Cliente OpenAI n√£o p√¥de ser inicializado pois a API Key n√£o est√° dispon√≠vel.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOoe2f8_h1UY",
        "outputId": "e6a3e028-3271-404f-97b9-984f18525427"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cliente OpenAI inicializado com sucesso para o modelo: gpt-4o-mini\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "3: Fun√ß√µes do Sistema RAG (semelhantes ao original)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "0ogcRskch39k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_information(query: str, user_profile: str) -> str:\n",
        "    \"\"\"\n",
        "    Simula a recupera√ß√£o de informa√ß√µes com base na consulta e no perfil do usu√°rio.\n",
        "    No Colab, esta fun√ß√£o permanece conceitualmente a mesma.\n",
        "    \"\"\"\n",
        "    context = \"\"\n",
        "    query_lower = query.lower()\n",
        "\n",
        "    # L√≥gica de recupera√ß√£o (mantida do seu c√≥digo original)\n",
        "    if \"enchente\" in query_lower:\n",
        "        if user_profile == \"V√≠tima\":\n",
        "            context = \"Situa√ß√£o de enchente: Procure um local alto e seguro. Evite contato com a √°gua da enchente. Sinalize sua localiza√ß√£o para equipes de resgate. Mantenha a calma. Telefones de emerg√™ncia: Defesa Civil 199, Bombeiros 193.\"\n",
        "            if \"preso\" in query_lower:\n",
        "                context += \" Se estiver preso, n√£o tente atravessar √°reas alagadas por conta pr√≥pria, aguarde o resgate. Se poss√≠vel, avise sobre sua condi√ß√£o e localiza√ß√£o exata.\"\n",
        "        elif user_profile == \"Morador\":\n",
        "            context = \"Alerta de enchente na regi√£o: Prepare um kit de emerg√™ncia (√°gua, comida, medicamentos, lanterna). Desligue a energia el√©trica se houver risco de a √°gua atingir sua casa. Mantenha-se informado pelos canais oficiais. Abrigos p√∫blicos: Gin√°sio Municipal, Escola Estadual Central.\"\n",
        "        elif user_profile == \"Familiar\":\n",
        "            context = \"Informa√ß√µes sobre enchentes: Para localizar familiares, entre em contato com a Defesa Civil (199) ou verifique listas em abrigos (Gin√°sio Municipal, Escola Estadual Central). Evite espalhar boatos, busque informa√ß√µes oficiais.\"\n",
        "    elif \"inc√™ndio florestal\" in query_lower:\n",
        "        if user_profile == \"V√≠tima\":\n",
        "            context = \"Situa√ß√£o de inc√™ndio florestal pr√≥ximo: Se receber ordem de evacua√ß√£o, saia imediatamente. Cubra nariz e boca com um pano √∫mido. Se n√£o puder sair, procure um local aberto e longe da vegeta√ß√£o densa. Ligue para 193 (Bombeiros).\"\n",
        "        elif user_profile == \"Morador\":\n",
        "            context = \"Alerta de inc√™ndio florestal: Mantenha a vegeta√ß√£o ao redor da casa limpa. Tenha rotas de fuga planejadas. Se avistar fuma√ßa ou focos de inc√™ndio, ligue para 193. N√£o fa√ßa queimadas.\"\n",
        "        elif user_profile == \"Familiar\":\n",
        "            context = \"Busca de informa√ß√µes sobre inc√™ndio: Contate autoridades locais para status da situa√ß√£o e informa√ß√µes sobre √°reas afetadas. Verifique not√≠cias de fontes confi√°veis.\"\n",
        "    else:\n",
        "        context = \"Informa√ß√µes gerais sobre seguran√ßa em desastres: Mantenha um kit de emerg√™ncia, tenha um plano familiar, mantenha-se informado por fontes oficiais e siga as orienta√ß√µes das autoridades.\"\n",
        "    return context\n",
        "\n",
        "def generate_augmented_response(user_query: str, retrieved_context: str, user_profile: str) -> str:\n",
        "    \"\"\"\n",
        "    Gera uma resposta aumentada usando o cliente OpenAI.\n",
        "    As mensagens de erro s√£o impressas no console do Colab.\n",
        "    \"\"\"\n",
        "    if not client:\n",
        "        return \"Erro: O cliente da API da OpenAI n√£o foi inicializado corretamente. Verifique sua API Key.\"\n",
        "\n",
        "    system_prompt = f\"\"\"Voc√™ √© um assistente virtual especializado em orientar pessoas durante desastres naturais.\n",
        "    Sua miss√£o √© fornecer informa√ß√µes claras, precisas, emp√°ticas e acion√°veis.\n",
        "    Voc√™ DEVE usar as \"Informa√ß√µes Relevantes Recuperadas\" para basear sua resposta.\n",
        "    N√ÉO invente informa√ß√µes que n√£o foram recuperadas, especialmente n√∫meros de telefone, endere√ßos espec√≠ficos ou detalhes factuais n√£o fornecidos no contexto.\n",
        "    Se as informa√ß√µes recuperadas n√£o forem totalmente suficientes, voc√™ pode complementar com conselhos gerais de seguran√ßa relevantes para o contexto do desastre mencionado e o perfil do usu√°rio, mas sempre indique claramente quando uma informa√ß√£o √© um conselho geral e n√£o parte dos dados recuperados.\n",
        "    Se a consulta for vaga, pe√ßa mais detalhes de forma gentil para poder ajudar melhor.\n",
        "    Responda diretamente √† pergunta do usu√°rio.\n",
        "    Priorize a seguran√ßa e o bem-estar. Seja emp√°tico, calmo e direto.\n",
        "    O perfil do usu√°rio atual √©: {user_profile}\"\"\"\n",
        "\n",
        "    user_content_prompt = f\"\"\"\n",
        "    **Consulta do Usu√°rio:**\n",
        "    \"{user_query}\"\n",
        "\n",
        "    **Informa√ß√µes Relevantes Recuperadas (use estas informa√ß√µes para basear sua resposta):**\n",
        "    \"{retrieved_context}\"\n",
        "\n",
        "    Com base no seu papel, no perfil do usu√°rio, na consulta e nas informa√ß√µes recuperadas, forne√ßa a orienta√ß√£o adequada:\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n--- CONSOLE DEBUG: IN√çCIO DE generate_augmented_response (OpenAI) ---\")\n",
        "    print(f\"CONSOLE DEBUG: Timestamp: {pd.Timestamp.now()}\")\n",
        "    print(f\"CONSOLE DEBUG: Usando OPENAI_MODEL_ID: {OPENAI_MODEL_ID}\")\n",
        "    print(f\"CONSOLE DEBUG: Comprimento System Prompt: {len(system_prompt)}, User Prompt: {len(user_content_prompt)}\")\n",
        "    print(\"---------------------------------------------------------\")\n",
        "\n",
        "    try:\n",
        "        print(f\"CONSOLE DEBUG: {pd.Timestamp.now()} - Tentando chamar OpenAI API com modelo de CHAT...\")\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=OPENAI_MODEL_ID,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_content_prompt}\n",
        "            ],\n",
        "            max_tokens=800,\n",
        "            temperature=0.4,\n",
        "            top_p=1.0,\n",
        "        )\n",
        "        full_response = response.choices[0].message.content.strip()\n",
        "\n",
        "        print(f\"CONSOLE DEBUG: {pd.Timestamp.now()} - Chamada √† API OpenAI (Chat) retornou.\")\n",
        "        print(f\"CONSOLE DEBUG: {pd.Timestamp.now()} - Resposta bruta do modelo (primeiros 300 chars): '{full_response[:300]}...'\")\n",
        "        print(f\"CONSOLE DEBUG: {pd.Timestamp.now()} - Comprimento total da resposta bruta: {len(full_response)}\")\n",
        "        if response.usage:\n",
        "            print(f\"CONSOLE DEBUG: Uso de tokens: {response.usage}\")\n",
        "        print(\"--- CONSOLE DEBUG: FIM DE generate_augmented_response (SUCESSO OpenAI) ---\\n\")\n",
        "        return full_response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"\\n!!!!!!!!!! OCORREU UMA EXCE√á√ÉO AO CHAMAR A API DA OPENAI (CONSOLE LOG) !!!!!!!!!!!\")\n",
        "        error_type_name = type(e).__name__\n",
        "        print(f\"CONSOLE DEBUG: Timestamp da Exce√ß√£o: {pd.Timestamp.now()}\")\n",
        "        print(f\"Tipo da Exce√ß√£o: {error_type_name}\")\n",
        "        print(f\"Mensagem da Exce√ß√£o: {str(e)}\")\n",
        "\n",
        "        # Tentar imprimir detalhes adicionais do erro, se dispon√≠veis\n",
        "        if hasattr(e, 'http_status'):\n",
        "            print(f\"HTTP Status: {e.http_status}\")\n",
        "        if hasattr(e, 'code'):\n",
        "            print(f\"OpenAI Error Code: {e.code}\")\n",
        "        if hasattr(e, 'param'):\n",
        "            print(f\"Error Param: {e.param}\")\n",
        "        if hasattr(e, 'body'): # Tentar mostrar o corpo do erro bruto\n",
        "            print(f\"Error Body: {e.body}\")\n",
        "        elif hasattr(e, 'json_body'): # Se o corpo for JSON\n",
        "             print(f\"Error JSON Body: {json.dumps(e.json_body, indent=2)}\")\n",
        "\n",
        "        print(\"--- CONSOLE DEBUG: FIM DE generate_augmented_response (EXCE√á√ÉO OpenAI) ---\\n\")\n",
        "        return \"Desculpe, ocorreu um problema ao tentar gerar sua resposta com a OpenAI. Verifique o console para mais detalhes sobre o erro.\"\n"
      ],
      "metadata": {
        "id": "oVJttei0h-U1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "4: Intera√ß√£o com o Usu√°rio e Execu√ß√£o Principal\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "_y32SYHviAjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_colab_interaction():\n",
        "    print(\"--- Assistente Virtual para Desastres Naturais (Vers√£o Colab) ---\")\n",
        "    print(f\"Utilizando o modelo: {OPENAI_MODEL_ID} (via OpenAI API)\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    if not client:\n",
        "        print(\"\\nAVISO: O cliente OpenAI n√£o est√° pronto. N√£o ser√° poss√≠vel gerar respostas do assistente.\")\n",
        "        print(\"Por favor, verifique sua API Key na C√©lula 2 e execute-a novamente.\")\n",
        "        return\n",
        "\n",
        "    # Sele√ß√£o de Perfil\n",
        "    user_profile_options = [\"V√≠tima\", \"Morador\", \"Familiar\"]\n",
        "    print(\"\\nüë§ Selecione seu perfil:\")\n",
        "    for i, option in enumerate(user_profile_options):\n",
        "        print(f\"{i+1}. {option}\")\n",
        "\n",
        "    profile_choice = \"\"\n",
        "    while not profile_choice.isdigit() or not (1 <= int(profile_choice) <= len(user_profile_options)):\n",
        "        profile_choice = input(f\"Digite o n√∫mero do seu perfil (1-{len(user_profile_options)}): \")\n",
        "        if not profile_choice.isdigit() or not (1 <= int(profile_choice) <= len(user_profile_options)):\n",
        "            print(\"Op√ß√£o inv√°lida. Por favor, tente novamente.\")\n",
        "    selected_profile = user_profile_options[int(profile_choice) - 1]\n",
        "    print(f\"Perfil selecionado: {selected_profile}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Entrada da Pergunta\n",
        "    user_query = input(\"‚ùì Descreva sua situa√ß√£o ou d√∫vida:\\n(Ex: Estou em uma √°rea de enchente e a √°gua est√° subindo. O que fa√ßo?)\\n\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    if not user_query:\n",
        "        print(\"‚ö†Ô∏è Voc√™ n√£o digitou uma pergunta. Tente novamente.\")\n",
        "        return\n",
        "\n",
        "    print(\"‚è≥ Processando sua solicita√ß√£o... Buscando informa√ß√µes e gerando orienta√ß√£o...\")\n",
        "\n",
        "    # 1. Recupera√ß√£o de Informa√ß√£o (Simulada)\n",
        "    print(\"\\n--- Contexto Recuperado (Informa√ß√µes de Apoio) ---\")\n",
        "    retrieved_info = retrieve_information(user_query, selected_profile)\n",
        "    if retrieved_info:\n",
        "        print(f\"DEBUG: Buscando informa√ß√µes para: '{user_query}' (Perfil: {selected_profile})...\")\n",
        "        print(f\"```\\n{retrieved_info}\\n```\")\n",
        "        print(f\"DEBUG: Contexto recuperado: {retrieved_info}\")\n",
        "    else:\n",
        "        print(\"Nenhum contexto espec√≠fico foi recuperado para esta consulta (usando informa√ß√µes gerais).\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # 2. Gera√ß√£o Aumentada\n",
        "    print(\"\\nüí° Orienta√ß√£o do Assistente:\")\n",
        "    final_response = generate_augmented_response(user_query, retrieved_info, selected_profile)\n",
        "    print(final_response)\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    print(\"\\n**Aten√ß√£o:** Este √© um assistente virtual para demonstra√ß√£o e apoio.\")\n",
        "    print(\"Em situa√ß√µes reais de emerg√™ncia, **sempre siga as orienta√ß√µes das autoridades locais**\")\n",
        "    print(\"e contate os servi√ßos de emerg√™ncia (Bombeiros: 193, Defesa Civil: 199, SAMU: 192).\")\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "id": "SByeB2jniEeG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "5: Executar a intera√ß√£o\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "7N01ZZxxiGlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\": # Garante que s√≥ execute se rodado como script principal\n",
        "    if client: # S√≥ executa a intera√ß√£o se o cliente OpenAI foi inicializado\n",
        "        main_colab_interaction()\n",
        "    else:\n",
        "        print(\"\\nO assistente n√£o pode ser executado porque o cliente OpenAI n√£o foi inicializado.\")\n",
        "        print(\"Verifique a configura√ß√£o da sua API Key na C√©lula 2 e execute-a novamente.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LokIlEEQiJvo",
        "outputId": "786e7459-db75-4c17-bb67-6a5c6057c519"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Assistente Virtual para Desastres Naturais (Vers√£o Colab) ---\n",
            "Utilizando o modelo: gpt-4o-mini (via OpenAI API)\n",
            "------------------------------------------------------------\n",
            "\n",
            "üë§ Selecione seu perfil:\n",
            "1. V√≠tima\n",
            "2. Morador\n",
            "3. Familiar\n",
            "Digite o n√∫mero do seu perfil (1-3): 2\n",
            "Perfil selecionado: Morador\n",
            "------------------------------------------------------------\n",
            "‚ùì Descreva sua situa√ß√£o ou d√∫vida:\n",
            "(Ex: Estou em uma √°rea de enchente e a √°gua est√° subindo. O que fa√ßo?)\n",
            "Estou em uma √°rea de enchente e a √°gua est√° subindo. O que fa√ßo?\n",
            "------------------------------------------------------------\n",
            "‚è≥ Processando sua solicita√ß√£o... Buscando informa√ß√µes e gerando orienta√ß√£o...\n",
            "\n",
            "--- Contexto Recuperado (Informa√ß√µes de Apoio) ---\n",
            "DEBUG: Buscando informa√ß√µes para: 'Estou em uma √°rea de enchente e a √°gua est√° subindo. O que fa√ßo?' (Perfil: Morador)...\n",
            "```\n",
            "Alerta de enchente na regi√£o: Prepare um kit de emerg√™ncia (√°gua, comida, medicamentos, lanterna). Desligue a energia el√©trica se houver risco de a √°gua atingir sua casa. Mantenha-se informado pelos canais oficiais. Abrigos p√∫blicos: Gin√°sio Municipal, Escola Estadual Central.\n",
            "```\n",
            "DEBUG: Contexto recuperado: Alerta de enchente na regi√£o: Prepare um kit de emerg√™ncia (√°gua, comida, medicamentos, lanterna). Desligue a energia el√©trica se houver risco de a √°gua atingir sua casa. Mantenha-se informado pelos canais oficiais. Abrigos p√∫blicos: Gin√°sio Municipal, Escola Estadual Central.\n",
            "------------------------------------------------------------\n",
            "\n",
            "üí° Orienta√ß√£o do Assistente:\n",
            "\n",
            "--- CONSOLE DEBUG: IN√çCIO DE generate_augmented_response (OpenAI) ---\n",
            "CONSOLE DEBUG: Timestamp: 2025-06-04 20:03:30.815235\n",
            "CONSOLE DEBUG: Usando OPENAI_MODEL_ID: gpt-4o-mini\n",
            "CONSOLE DEBUG: Comprimento System Prompt: 970, User Prompt: 607\n",
            "---------------------------------------------------------\n",
            "CONSOLE DEBUG: 2025-06-04 20:03:30.815503 - Tentando chamar OpenAI API com modelo de CHAT...\n",
            "CONSOLE DEBUG: 2025-06-04 20:03:35.489278 - Chamada √† API OpenAI (Chat) retornou.\n",
            "CONSOLE DEBUG: 2025-06-04 20:03:35.489798 - Resposta bruta do modelo (primeiros 300 chars): '√â muito importante agir rapidamente em situa√ß√µes de enchente. Aqui est√£o algumas orienta√ß√µes que voc√™ deve seguir:\n",
            "\n",
            "1. **Prepare um kit de emerg√™ncia**: Certifique-se de ter √°gua pot√°vel, alimentos n√£o perec√≠veis, medicamentos necess√°rios, uma lanterna e pilhas extras.\n",
            "\n",
            "2. **Desligue a energia el√©tr...'\n",
            "CONSOLE DEBUG: 2025-06-04 20:03:35.489852 - Comprimento total da resposta bruta: 975\n",
            "CONSOLE DEBUG: Uso de tokens: CompletionUsage(completion_tokens=220, prompt_tokens=344, total_tokens=564, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
            "--- CONSOLE DEBUG: FIM DE generate_augmented_response (SUCESSO OpenAI) ---\n",
            "\n",
            "√â muito importante agir rapidamente em situa√ß√µes de enchente. Aqui est√£o algumas orienta√ß√µes que voc√™ deve seguir:\n",
            "\n",
            "1. **Prepare um kit de emerg√™ncia**: Certifique-se de ter √°gua pot√°vel, alimentos n√£o perec√≠veis, medicamentos necess√°rios, uma lanterna e pilhas extras.\n",
            "\n",
            "2. **Desligue a energia el√©trica**: Se voc√™ perceber que a √°gua pode atingir sua casa, desligue a energia el√©trica para evitar riscos de choque el√©trico.\n",
            "\n",
            "3. **Mantenha-se informado**: Acompanhe as atualiza√ß√µes sobre a enchente pelos canais oficiais, como r√°dio, televis√£o ou aplicativos de not√≠cias.\n",
            "\n",
            "4. **Considere evacuar**: Se a situa√ß√£o estiver se tornando cr√≠tica, procure um abrigo p√∫blico. Os locais dispon√≠veis s√£o o Gin√°sio Municipal e a Escola Estadual Central. \n",
            "\n",
            "5. **Evite √°reas alagadas**: N√£o tente atravessar √°reas inundadas a p√© ou de carro, pois isso pode ser muito perigoso.\n",
            "\n",
            "Priorize sua seguran√ßa e a de sua fam√≠lia. Se precisar de mais ajuda ou informa√ß√µes, n√£o hesite em perguntar.\n",
            "------------------------------------------------------------\n",
            "\n",
            "**Aten√ß√£o:** Este √© um assistente virtual para demonstra√ß√£o e apoio.\n",
            "Em situa√ß√µes reais de emerg√™ncia, **sempre siga as orienta√ß√µes das autoridades locais**\n",
            "e contate os servi√ßos de emerg√™ncia (Bombeiros: 193, Defesa Civil: 199, SAMU: 192).\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}