# app.py
import streamlit as st
import os
from dotenv import load_dotenv
from openai import OpenAI
import json
import pandas as pd

# --- Configura√ß√µes Iniciais ---
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Modelo da OpenAI que voc√™ deseja usar
OPENAI_MODEL_ID = "gpt-4o-mini" # Usando o modelo GPT-4o mini

# Inicializar o cliente da API da OpenAI
if not OPENAI_API_KEY:
    st.error("Erro Cr√≠tico: A API Key da OpenAI (OPENAI_API_KEY) n√£o foi encontrada no arquivo .env.")
    st.stop()

try:
    client = OpenAI(api_key=OPENAI_API_KEY)
except Exception as e:
    st.error(f"Erro ao inicializar o cliente da API da OpenAI: {e}")
    st.stop()

# --- Fun√ß√µes do Sistema RAG ---
def retrieve_information(query: str, user_profile: str) -> str:
    context = ""
    query_lower = query.lower()
    if "enchente" in query_lower:
        if user_profile == "V√≠tima":
            context = "Situa√ß√£o de enchente: Procure um local alto e seguro. Evite contato com a √°gua da enchente. Sinalize sua localiza√ß√£o para equipes de resgate. Mantenha a calma. Telefones de emerg√™ncia: Defesa Civil 199, Bombeiros 193."
            if "preso" in query_lower:
                context += " Se estiver preso, n√£o tente atravessar √°reas alagadas por conta pr√≥pria, aguarde o resgate. Se poss√≠vel, avise sobre sua condi√ß√£o e localiza√ß√£o exata."
        elif user_profile == "Morador":
            context = "Alerta de enchente na regi√£o: Prepare um kit de emerg√™ncia (√°gua, comida, medicamentos, lanterna). Desligue a energia el√©trica se houver risco de a √°gua atingir sua casa. Mantenha-se informado pelos canais oficiais. Abrigos p√∫blicos: Gin√°sio Municipal, Escola Estadual Central."
        elif user_profile == "Familiar":
            context = "Informa√ß√µes sobre enchentes: Para localizar familiares, entre em contato com a Defesa Civil (199) ou verifique listas em abrigos (Gin√°sio Municipal, Escola Estadual Central). Evite espalhar boatos, busque informa√ß√µes oficiais."
    elif "inc√™ndio florestal" in query_lower:
        if user_profile == "V√≠tima":
            context = "Situa√ß√£o de inc√™ndio florestal pr√≥ximo: Se receber ordem de evacua√ß√£o, saia imediatamente. Cubra nariz e boca com um pano √∫mido. Se n√£o puder sair, procure um local aberto e longe da vegeta√ß√£o densa. Ligue para 193 (Bombeiros)."
        elif user_profile == "Morador":
            context = "Alerta de inc√™ndio florestal: Mantenha a vegeta√ß√£o ao redor da casa limpa. Tenha rotas de fuga planejadas. Se avistar fuma√ßa ou focos de inc√™ndio, ligue para 193. N√£o fa√ßa queimadas."
        elif user_profile == "Familiar":
            context = "Busca de informa√ß√µes sobre inc√™ndio: Contate autoridades locais para status da situa√ß√£o e informa√ß√µes sobre √°reas afetadas. Verifique not√≠cias de fontes confi√°veis."
    else:
        context = "Informa√ß√µes gerais sobre seguran√ßa em desastres: Mantenha um kit de emerg√™ncia, tenha um plano familiar, mantenha-se informado por fontes oficiais e siga as orienta√ß√µes das autoridades."
    return context

def generate_augmented_response(user_query: str, retrieved_context: str, user_profile: str) -> str:
    system_prompt = f"""Voc√™ √© um assistente virtual especializado em orientar pessoas durante desastres naturais.
    Sua miss√£o √© fornecer informa√ß√µes claras, precisas, emp√°ticas e acion√°veis.
    Voc√™ DEVE usar as "Informa√ß√µes Relevantes Recuperadas" para basear sua resposta.
    N√ÉO invente informa√ß√µes que n√£o foram recuperadas, especialmente n√∫meros de telefone, endere√ßos espec√≠ficos ou detalhes factuais n√£o fornecidos no contexto.
    Se as informa√ß√µes recuperadas n√£o forem totalmente suficientes, voc√™ pode complementar com conselhos gerais de seguran√ßa relevantes para o contexto do desastre mencionado e o perfil do usu√°rio, mas sempre indique claramente quando uma informa√ß√£o √© um conselho geral e n√£o parte dos dados recuperados.
    Se a consulta for vaga, pe√ßa mais detalhes de forma gentil para poder ajudar melhor.
    Responda diretamente √† pergunta do usu√°rio.
    Priorize a seguran√ßa e o bem-estar. Seja emp√°tico, calmo e direto.
    O perfil do usu√°rio atual √©: {user_profile}"""

    user_content_prompt = f"""
    **Consulta do Usu√°rio:**
    "{user_query}"

    **Informa√ß√µes Relevantes Recuperadas (use estas informa√ß√µes para basear sua resposta):**
    "{retrieved_context}"

    Com base no seu papel, no perfil do usu√°rio, na consulta e nas informa√ß√µes recuperadas, forne√ßa a orienta√ß√£o adequada:
    """

    print("\n--- CONSOLE DEBUG: IN√çCIO DE generate_augmented_response (OpenAI) ---")
    print(f"CONSOLE DEBUG: Timestamp: {pd.Timestamp.now()}")
    print(f"CONSOLE DEBUG: Usando OPENAI_MODEL_ID: {OPENAI_MODEL_ID}")
    print(f"CONSOLE DEBUG: Comprimento System Prompt: {len(system_prompt)}, User Prompt: {len(user_content_prompt)}")
    print("---------------------------------------------------------")

    try:
        print(f"CONSOLE DEBUG: {pd.Timestamp.now()} - Tentando chamar OpenAI API com modelo de CHAT...")
        
        response = client.chat.completions.create(
            model=OPENAI_MODEL_ID,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content_prompt}
            ],
            max_tokens=800, 
            temperature=0.4, 
            top_p=1.0,
        )
        full_response = response.choices[0].message.content.strip()
        
        print(f"CONSOLE DEBUG: {pd.Timestamp.now()} - Chamada √† API OpenAI (Chat) retornou.")
        print(f"CONSOLE DEBUG: {pd.Timestamp.now()} - Resposta bruta do modelo (primeiros 300 chars): '{full_response[:300]}...'")
        print(f"CONSOLE DEBUG: {pd.Timestamp.now()} - Comprimento total da resposta bruta: {len(full_response)}")
        if response.usage:
            print(f"CONSOLE DEBUG: Uso de tokens: {response.usage}")
        print("--- CONSOLE DEBUG: FIM DE generate_augmented_response (SUCESSO OpenAI) ---\n")
        return full_response

    except Exception as e:
        st.error("!!!!!!!!!! OCORREU UMA EXCE√á√ÉO AO CHAMAR A API DA OPENAI (DETALHES NA UI ABAIXO) !!!!!!!!!!!")
        error_type_name = type(e).__name__
        st.error(f"Tipo da Exce√ß√£o: {error_type_name}")
        st.error(f"Mensagem da Exce√ß√£o: {str(e)}")

        print("\n!!!!!!!!!! OCORREU UMA EXCE√á√ÉO AO CHAMAR A API DA OPENAI (CONSOLE LOG) !!!!!!!!!!!")
        print(f"CONSOLE DEBUG: Timestamp da Exce√ß√£o: {pd.Timestamp.now()}")
        print(f"Tipo da Exce√ß√£o: {error_type_name}")
        print(f"Mensagem da Exce√ß√£o: {str(e)}")

        if hasattr(e, 'http_status'):
            st.error(f"HTTP Status: {e.http_status}")
            print(f"HTTP Status: {e.http_status}")
        if hasattr(e, 'code'):
            st.error(f"OpenAI Error Code: {e.code}")
            print(f"OpenAI Error Code: {e.code}")
        if hasattr(e, 'param'):
            st.error(f"Error Param: {e.param}")
            print(f"Error Param: {e.param}")
        if hasattr(e, 'body'):
            st.error(f"Error Body: {e.body}") # Tentar mostrar o corpo do erro bruto
            print(f"Error Body: {e.body}")
        elif hasattr(e, 'json_body'): # Se o corpo for JSON
             st.error(f"Error JSON Body: {json.dumps(e.json_body, indent=2)}")
             print(f"Error JSON Body: {json.dumps(e.json_body, indent=2)}")
        
        print("--- CONSOLE DEBUG: FIM DE generate_augmented_response (EXCE√á√ÉO OpenAI) ---\n")
        return "Desculpe, ocorreu um problema ao tentar gerar sua resposta com a OpenAI. Verifique a interface e o console para mais detalhes sobre o erro."

# --- Interface Streamlit ---
st.set_page_config(page_title="Assistente de Desastres", layout="wide")

st.title("üÜò Assistente Virtual para Desastres Naturais")
st.caption(f"Utilizando o modelo: {OPENAI_MODEL_ID} (via OpenAI API)")

st.sidebar.header("üë§ Perfil do Usu√°rio")
user_profile_options = ["V√≠tima", "Morador", "Familiar"]
selected_profile = st.sidebar.selectbox(
    "Selecione seu perfil para receber orienta√ß√£o adequada:",
    user_profile_options,
    index=0 
)

st.info(
    f"**Voc√™ est√° interagindo como: {selected_profile}**\n"
    "A orienta√ß√£o ser√° adaptada a este perfil."
)

user_query = st.text_area("‚ùì Descreva sua situa√ß√£o ou d√∫vida:", height=150, placeholder="Ex: Estou em uma √°rea de enchente e a √°gua est√° subindo. O que fa√ßo?")

if st.button("Enviar Pergunta e Obter Orienta√ß√£o", type="primary", use_container_width=True):
    if not user_query:
        st.warning("Por favor, digite sua pergunta para que eu possa ajudar.")
    else:
        with st.spinner("Processando sua solicita√ß√£o... Buscando informa√ß√µes e gerando orienta√ß√£o..."):
            st.subheader("Contexto Recuperado (Informa√ß√µes de Apoio):")
            with st.expander("Clique para ver os dados recuperados (simulado)", expanded=False):
                st.write(f"DEBUG UI: Buscando informa√ß√µes para: '{user_query}' (Perfil: {selected_profile})...")
                retrieved_info = retrieve_information(user_query, selected_profile)
                if retrieved_info:
                    st.markdown(f"```\n{retrieved_info}\n```")
                    st.write(f"DEBUG UI: Contexto recuperado: {retrieved_info}")
                else:
                    st.caption("Nenhum contexto espec√≠fico foi recuperado para esta consulta.")

            st.subheader("üí° Orienta√ß√£o do Assistente:")
            final_response = generate_augmented_response(user_query, retrieved_info, selected_profile)
            st.markdown(final_response)

st.sidebar.markdown("---")
st.sidebar.warning(
    "**Aten√ß√£o:** Este √© um assistente virtual para demonstra√ß√£o e apoio.\n\n"
    "Em situa√ß√µes reais de emerg√™ncia, **sempre siga as orienta√ß√µes das autoridades locais** "
    "e contate os servi√ßos de emerg√™ncia (Bombeiros: 193, Defesa Civil: 199, SAMU: 192)."
)